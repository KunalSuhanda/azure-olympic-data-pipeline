# **Azure Olympic Data Pipeline** 🏅🚀
📌 **An end-to-end Data Engineering project using Azure services: Data Factory, Databricks, Synapse Analytics, and Tableau.**

---

## 📌 **Project Overview**
This project demonstrates an **end-to-end Data Engineering pipeline** built on **Azure Cloud**, processing and analyzing **Olympic Games data** using various services.

### 🔹 **Workflow Summary:**
✅ **Extract** raw Olympic data from Kaggle/GitHub using **Azure Data Factory**  
✅ **Transform & Clean** the data using **Azure Databricks (Apache Spark)**  
✅ **Load & Store** the refined data in **Azure Synapse Analytics**  
✅ **Analyze & Visualize** insights using **SQL Queries & Tableau Dashboard**  

📊 **Final Deliverable**: An **interactive Tableau dashboard** showcasing **Olympic athlete statistics, medal counts, and country-wise performance trends**.

---

## ⚡ **Project Architecture**

![Image 2-18-25 at 3 07 PM](https://github.com/user-attachments/assets/1cf2a048-9179-4e9f-bc24-13e12e0fe79f)

---

## 🏗️ **Tech Stack & Tools Used**
| **Category**     | **Tools/Technologies**  |
|-----------------|-----------------------|
| **Cloud Platform** | Microsoft Azure ☁️ |
| **Data Ingestion** | Azure Data Factory 🔄 |
| **Data Processing** | Azure Databricks (Apache Spark) 🔥 |
| **Data Storage** | Azure Data Lake Storage 📦 |
| **Data Warehousing** | Azure Synapse Analytics 🗄️ |
| **Data Visualization** | Tableau 📊 |
| **Programming Language** | Python 🐍, SQL 📜 |

---

## 🔥 **Key Features**
🔹 **Automated Data Pipeline**: Data ingestion & transformation using **Azure Data Factory + Databricks**  
🔹 **Optimized Data Processing**: Apache Spark for **scalable & fast processing**  
🔹 **Cloud-Based Analytics**: Use of **Azure Synapse** to perform SQL-based analysis  
🔹 **Interactive Dashboards**: Insights **visualized on Tableau**  
🔹 **Reproducibility**: Can be scaled for **real-time data pipelines**  

---

## 🚀 **Project Workflow**
### 📥 **Step 1: Data Extraction**
- **Source:** Kaggle Dataset (Olympics Data)  
- **Method:** Extracted using **Azure Data Factory** & stored in **Azure Data Lake Storage**

### 🔄 **Step 2: Data Transformation**
- **Tools Used:** **Azure Databricks** (Apache Spark)  
- **Operations:**  
  ✔️ Cleaned raw data (removed null values, duplicates)  
  ✔️ Standardized column formats  
  ✔️ Aggregated important statistics  

![Image 2-18-25 at 3 07 PM (1)](https://github.com/user-attachments/assets/6e00ea35-7a61-4f0d-b117-df3763fe460c)
![Image 2-18-25 at 2 59 PM](https://github.com/user-attachments/assets/c0ce834f-1a9c-4154-91eb-23378a449069)

### 🏛 **Step 3: Data Warehousing**
- **Storage Location:** Azure Data Lake Storage  
- **Query Engine:** Azure Synapse Analytics (SQL)  

![Image 2-18-25 at 3 00 PM](https://github.com/user-attachments/assets/e72e3585-606c-4425-a3af-440a7989fd58)

![Image 2-18-25 at 3 09 PM](https://github.com/user-attachments/assets/492d1fda-1b26-4e9f-8787-6cfb47f9c88a)

### 📊 **Step 4: Data Visualization**

- **Visualization Tool:** **Tableau**  
- **Insights Generated:**  
  ✔️ Top countries by athlete count  
  ✔️ Medal distribution by country  
  ✔️ Sport-wise participation statistics  

---

## 📊 **Tableau Dashboard Preview**
![Olympic Athletes Dashboard](https://github.com/user-attachments/assets/bd2b59ed-1c14-4e6c-985b-658a54dc3b95)



---

## 🛠️ **How to Set Up & Run the Project**
### 🔹 **Prerequisites**
- An **Azure Subscription** with access to **Azure Data Factory, Databricks, Synapse Analytics**
- **Python & SQL knowledge** (basic)
- **Tableau Desktop** (for visualization)

### 📌 **Setup Steps**
1️⃣ **Clone this repository**  
```bash
git clone https://github.com/KunalSuhanda/azure-olympic-data-pipeline.git
cd azure-olympic-data-pipeline
